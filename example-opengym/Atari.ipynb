{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an issue with the renders in Visual studio jupyter that shuts down the kernel when you force quit the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import warnings\n",
    "from collections import deque\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.25.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can opt not to use `render_mode='human'` when you initialize the `env`.  \n",
    "This will stop the game window from appearing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BreakoutNoFrameskip-v4\")\n",
    "# env = gym.make(\"BreakoutNoFrameskip-v4\", render_mode='human')\n",
    "\n",
    "print(\"Observation Space: \", env.observation_space.shape)\n",
    "print(\"Action Space       \", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our observation space is a continuous space of dimensions (210, 160, 3) corresponding to an RGB pixel observation of the same size.  \n",
    "Our action space  contains 4 discrete actions (Left, Right, Do Nothing, Fire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "score = 0\n",
    "num_episodes = 5\n",
    "for i_episode in range(num_episodes):\n",
    "    for i in range(1000):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        time.sleep(0.01)\n",
    "        if done:\n",
    "            print(\"\\rEpisode:{} Score:{}\".format(i_episode, score), end=\"\")\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our observation space is a continuous space of dimensions (210, 160, 3) corresponding to an RGB pixel observation of the same size. Our action space  contains 4 discrete actions (Left, Right, Do Nothing, Fire)\n",
    "\n",
    "Now that we have our environment loaded, let us suppose we have to make certain changes to the Atari Environment. It's a common practice in Deep RL that we construct our observation by concatenating the past `k` frames together. We have to modify the Breakout Environment such that both our `reset` and `step` functions return concatenated observations.\n",
    "\n",
    "For this we define a class of type `gym.Wrapper` to override the `reset` and `return` functions of the Breakout `Env`. The `Wrapper` class, as the name suggests, is a wrapper on top of an `Env` class that modifies some of its attributes and functions.\n",
    "\n",
    "The `__init__` function is defined with the `Env` class for which the wrapper is written, and the number of past frames to be concatenated. Note that we also need to redefine the observation space since we are now using concatenated frames as our observations. (We modify the observation space from (210, 160, 3) to (210, 160, 3 * num_past_frames.)\n",
    "\n",
    "In the `reset` function, while we are initializing the environment, since we don't have any previous observations to concatenate, we concatenate just the initial observations repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers.atari_preprocessing import AtariPreprocessing\n",
    "\n",
    "class ConcatObs(gym.Wrapper):\n",
    "    def __init__(self, env, k):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = \\\n",
    "            spaces.Box(low=0, high=255, shape=((k,) + shp), dtype=env.observation_space.dtype)\n",
    "\n",
    "def reset(self):\n",
    "    ob = self.env.reset()\n",
    "    print(ob.shape)\n",
    "    for _ in range(self.k):\n",
    "        self.frames.append(ob)\n",
    "    return 5\n",
    "\n",
    "def step(self, action):\n",
    "    ob, reward, done, info = self.env.step(action)\n",
    "    self.frames.append(ob)\n",
    "    return self._get_ob(), reward, done, info\n",
    "\n",
    "def _get_ob(self):\n",
    "    print(\"_get_ob\")\n",
    "    return np.array(self.frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new observation space is Box(0, 255, (4, 210, 160, 3), uint8)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"BreakoutNoFrameskip-v4\")\n",
    "wrapped_env = ConcatObs(env, 4)\n",
    "print(\"The new observation space is\", wrapped_env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now verify whether the observations are indeed concatenated or not.\n",
    "> I'm not sure why its not displaying (4, 210, 160, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (4, 210, 160, 3), uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intial obs is of the shape (210, 160, 3)\n",
      "Obs after taking a step is (210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "# Reset the Env\n",
    "obs = wrapped_env.reset()\n",
    "print(\"Intial obs is of the shape\", obs.shape)\n",
    "\n",
    "# Take one step\n",
    "obs, _, _, _  = wrapped_env.step(0)\n",
    "print(\"Obs after taking a step is\", obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is more to Wrappers than the vanilla Wrapper class. Gym also provides you with specific wrappers that target specific elements of the environment, such as observations, rewards, and actions. Their use is demonstrated in the following section.\n",
    "\n",
    "`ObservationWrapper`: This helps us make changes to the observation using the observation method of the wrapper class.  \n",
    "`RewardWrapper`: This helps us make changes to the reward using the reward function of the wrapper class.  \n",
    "`ActionWrapper`: This helps us make changes to the action using the action function of the wrapper class.  \n",
    "\n",
    "\n",
    "Let us suppose that we have to make the follow changes to our environment:\n",
    "\n",
    "* We have to normalize the pixel observations by 255.\n",
    "* We have to clip the rewards between 0 and 1.\n",
    "* We have to prevent the slider from moving to the left (action 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "class ObservationWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def observation(self, obs):\n",
    "        # Normalise observation by 255\n",
    "        return obs / 255.0\n",
    "\n",
    "class RewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def reward(self, reward):\n",
    "        # Clip reward between 0 to 1\n",
    "        return np.clip(reward, 0, 1)\n",
    "    \n",
    "class ActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def action(self, action):\n",
    "        if action == 3:\n",
    "            return random.choice([0,1,2])\n",
    "        else:\n",
    "            return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All checks passed\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"BreakoutNoFrameskip-v4\", render_mode='human')\n",
    "wrapped_env = ObservationWrapper(RewardWrapper(ActionWrapper(env)))\n",
    "\n",
    "obs = wrapped_env.reset()\n",
    "\n",
    "for step in range(500):\n",
    "    action = wrapped_env.action_space.sample()\n",
    "    obs, reward, done, info = wrapped_env.step(action)\n",
    "    \n",
    "    # Raise a flag if values have not been vectorised properly\n",
    "    if (obs > 1.0).any() or (obs < 0.0).any():\n",
    "        print(\"Max and min value of observations out of range\")\n",
    "    \n",
    "    # Raise a flag if reward has not been clipped.\n",
    "    if reward < 0.0 or reward > 1.0:\n",
    "        assert False, \"Reward out of bounds\"\n",
    "    \n",
    "    # Check the rendering if the slider moves to the left.\n",
    "    time.sleep(0.001)\n",
    "\n",
    "wrapped_env.close()\n",
    "\n",
    "print(\"All checks passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapped Env: <ObservationWrapper<RewardWrapper<ActionWrapper<TimeLimit<OrderEnforcing<StepAPICompatibility<PassiveEnvChecker<AtariEnv<BreakoutNoFrameskip-v4>>>>>>>>>\n",
      "Unwrapped Env <AtariEnv<BreakoutNoFrameskip-v4>>\n",
      "Getting the meaning of actions ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
     ]
    }
   ],
   "source": [
    "print(\"Wrapped Env:\", wrapped_env)\n",
    "print(\"Unwrapped Env\", wrapped_env.unwrapped)\n",
    "print(\"Getting the meaning of actions\", wrapped_env.unwrapped.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('vu_tutorial')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cebd3645ccbf8347f836071505ea8392e8108cc313a83e2a45c880143924a13b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
