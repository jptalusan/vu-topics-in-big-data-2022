{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import helper\n",
    "import imageio\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialisation: Create and initialise the environment.\n",
    "\n",
    "2. Execution: Take repeated actions in the environment. At each step the environment provides information to describe its new state and the reward received as a consequence of taking the specified action. This continues until the environment signals that the episode is complete.\n",
    "\n",
    "3. Termination: Cleanup and destroy the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.25.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "* Example was created using `gym` version `0.25.1`\n",
    "* Don't mind the deprecation warning, all the examples I saw used `new_step_api=False`. Couldn't find documentation that uses the new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josepaolo-t/anaconda3/envs/vu_tutorial/lib/python3.9/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/Users/josepaolo-t/anaconda3/envs/vu_tutorial/lib/python3.9/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0', new_step_api=False, render_mode='human')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic structure of the environment is described by the `observation_space` and the `action_space` attributes of the Gym `Env` class.\n",
    "The `observation_space` defines the structure as well as the legitimate values for the observation of the state of the environment. The observation can be different things for different environments. The most common form is a screenshot of the game. There can be other forms of observations as well, such as certain characteristics of the environment described in vector form.\n",
    "\n",
    "Similarly, the `Env` class also defines an attribute called the action_space, which describes the numerical structure of the legitimate actions that can be applied to the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The observation space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "The action space: Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "# Observation and action space \n",
    "obs_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "print(\"The observation space: {}\".format(obs_space))\n",
    "print(\"The action space: {}\".format(action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we cover functions of the Env class that help the agent interact with the environment. Two such important functions are:\n",
    "\n",
    "* `reset`: This function resets the environment to its initial state, and returns the observation of the environment corresponding to the initial state.\n",
    "* `step` : This function takes an action as an input and applies it to the environment, which leads to the environment transitioning to a new state. The reset function returns four things:\n",
    "    * `observation`: The observation of the state of the environment.\n",
    "    * `reward`: The reward that you can get from the environment after executing the action that was given as the input to the step function.\n",
    "    * `done`: Whether the episode has been terminated. If true, you may need to end the simulation or reset the environment to restart the episode.\n",
    "    * `info`: This provides additional information depending on the environment, such as number of lives left, or general information that may be conducive in debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`env.render()` will allow you to view the environment in a separate window if you are using visual studio code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial observation is [-0.47815958  0.        ]\n",
      "The new observation is [-0.4774993   0.00066026]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josepaolo-t/anaconda3/envs/vu_tutorial/lib/python3.9/site-packages/gym/core.py:51: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0', new_step_api=False, render_mode='human')\n",
    "\n",
    "# reset the environment and see the initial observation\n",
    "obs = env.reset()\n",
    "print(\"The initial observation is {}\".format(obs))\n",
    "\n",
    "# show the environment\n",
    "env.render(mode=\"human\")\n",
    "\n",
    "# Sample a random action from the entire action space\n",
    "random_action = env.action_space.sample()\n",
    "\n",
    "# # Take the action and get the new observation space\n",
    "new_obs, reward, done, info = env.step(random_action)\n",
    "print(\"The new observation is {}\".format(new_obs))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show the observation as a screenshot:\n",
    "env = gym.make('MountainCar-v0', new_step_api=False, render_mode='rgb_array')\n",
    "\n",
    "# reset the environment and see the initial observation\n",
    "obs = env.reset()\n",
    "print(\"The initial observation is {}\".format(obs))\n",
    "\n",
    "# show the environment\n",
    "env_screen = env.render(mode = 'rgb_array')\n",
    "\n",
    "# Sample a random action from the entire action space\n",
    "random_action = env.action_space.sample()\n",
    "\n",
    "# # Take the action and get the new observation space\n",
    "new_obs, reward, done, info = env.step(random_action)\n",
    "print(\"The new observation is {}\".format(new_obs))\n",
    "\n",
    "env.close()\n",
    "\n",
    "# This displays a single screen shot\n",
    "plt.imshow(env_screen[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different methods to render and display environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can work but too slow\n",
    "# from IPython import display\n",
    "# %matplotlib inline\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# num_episodes=40\n",
    "# for i_episode in range(num_episodes):\n",
    "#     observation = env.reset()\n",
    "#     for t in range(500):\n",
    "#         plt.imshow(env.render(mode='rgb_array'))\n",
    "#         display.display(plt.gcf())\n",
    "#         display.clear_output(wait=True)\n",
    "#         env.step(env.action_space.sample()) # take a random action\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of steps you run the agent for \n",
    "num_steps = 1500\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "num_episodes = 40\n",
    "for i_episode in range(num_episodes):\n",
    "    obs = env.reset()\n",
    "    for step in range(num_steps):\n",
    "        # take random action, but you can also do something more intelligent\n",
    "        # action = my_intelligent_agent_fn(obs) \n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        # apply the action\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Render the env\n",
    "        env.render(mode='human')\n",
    "\n",
    "        # Wait a bit before the next frame unless you want to see a crazy fast video\n",
    "        time.sleep(0.001)\n",
    "        \n",
    "        # If the epsiode is up, then start another one\n",
    "        if done:\n",
    "            print(\"\\rEpisode {}/{} finished after {} timesteps\".format(i_episode+1, num_episodes, t+1), end=\"\")\n",
    "            break\n",
    "\n",
    "# Close the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rendering on new pygame window\n",
    "env = gym.make('MountainCar-v0')\n",
    "cum_reward = 0\n",
    "num_episodes=40\n",
    "for i_episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    for t in range(500):\n",
    "        # Render into buffer.\n",
    "        action = env.action_space.sample() # random action\n",
    "        env.render()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"\\rEpisode {}/{} finished after {} timesteps\".format(i_episode+1, num_episodes, t+1), end=\"\")\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving to a gif (can take a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "cum_reward = 0\n",
    "frames = []\n",
    "num_episodes=40\n",
    "for i_episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    for t in range(500):\n",
    "        # Render into buffer.\n",
    "        action = env.action_space.sample() # random action\n",
    "        frame = env.render(mode='rgb_array')\n",
    "        frames.append(helper._label_with_episode_number(frame, episode_num=i_episode))\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"\\rEpisode {}/{} finished after {} timesteps\".format(i_episode+1, num_episodes, t+1), end=\"\")\n",
    "            break\n",
    "env.close()\n",
    "imageio.mimwrite(os.path.join('./videos/', 'random_agent.gif'), frames, fps=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spaces\n",
    "The `observation_space` for our environment was `Box(2,)`, and the `action_space` was `Discrete(2,)`. What do these actually mean? Both Box and Discrete are types of data structures called \"Spaces\" provided by Gym to describe the legitimate values for the observations and actions for the environments.\n",
    "\n",
    "All of these data structures are derived from the gym.Space base class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Box(n,)` corresponds to the n-dimensional continuous space. In our case `n=2`, thus the observational space of our environment is a 2-D space. Of course, the space is bounded by upper and lower limits which describe the legitimate values our observations can take. We can determine this using the high and low attributes of the observation space. These correspond to the maximum and minimum positions/velocities in our environment, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Upper Bound for Env Observation\", env.observation_space.high)\n",
    "print(\"Lower Bound for Env Observation\", env.observation_space.low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discrete `action_space` are the actions that the agent can take. In the `MountainCar` example, the car can do 1 of 3 things, `[stop]`, `[move left]` and `[move right]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = env.action_space\n",
    "print(\"The action space: {}\".format(action_space))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('vu_tutorial')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cebd3645ccbf8347f836071505ea8392e8108cc313a83e2a45c880143924a13b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
