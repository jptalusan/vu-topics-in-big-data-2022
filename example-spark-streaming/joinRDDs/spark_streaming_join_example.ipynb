{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import json\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    " \n",
    "'''\n",
    "Join the batting and salaries data for Barry Bonds per year.\n",
    " \n",
    "The output should be the combined CSV string of batting and salaries data (one per year).\n",
    " \n",
    "Use 'join' as the key for the final output in the reducer.\n",
    " \n",
    "E.g:\n",
    "\"join\"  \"bondsba01,1986,1,PIT,NL,113,413,72,92,26,3,16,48,36,7,65,102,2,2,2,2,4,1986,PIT,NL,bondsba01,60000\"\n",
    " \n",
    "Schema:\n",
    "Salaries: yearID  teamID  lgID  playerID  salary\n",
    "Batting: playerID yearID  stint teamID  lgID  G AB  R H 2B  3B  HR  RBI SB  CS  BB  SO\n",
    " \n",
    "Hints: \n",
    "Use split to split the CSV lines (e.g. s = line.split(','))\n",
    "Both files are sent to the mapper. Use the length of the lines to determine which is which.\n",
    "'''\n",
    "if __name__ == '__main__':\n",
    "    # Create Spark context\n",
    "    conf = SparkConf().setAppName(\"4_join\").set(\"spark.streaming.concurrentJobs\", \"2\").set('spark.hadoop.validateOutputSpecs', False)\n",
    "    sc = SparkContext(conf=conf)\n",
    "    sc.setLogLevel(\"WARN\")\n",
    " \n",
    "    # Create Spark Streaming context\n",
    "    ssc = StreamingContext(sparkContext=sc, batchDuration=1)\n",
    " \n",
    "    # Defining the checkpoint directory\n",
    "    ssc.checkpoint(\"/root/tmp\")\n",
    " \n",
    "    # Connect to Kafka and subscribe two topics\n",
    "    kafkaStream = KafkaUtils.createDirectStream(ssc=ssc,\n",
    "                                                kafkaParams={\"metadata.broker.list\": '<your ec2 instance public IP>:9092'},\n",
    "                                                topics=['4_join_batting', '4_join_salaries'])\n",
    "    \n",
    "    '''\n",
    "    Steps of joining samples from Salaries and Batting tables:\n",
    "    1. filter samples by playerID: bondsba01\n",
    "    2. determine whether a sample is from Salaries table or Batting table\n",
    "    3. generate (key, value) pairs where yearID is the key\n",
    " \n",
    "    ----- Once the above steps are done, we can start joining samples...\n",
    "    \n",
    "    What does updateStateByKey do?\n",
    "    Return a new \"state\" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values for the key. This can be used to maintain arbitrary state data for each key.\n",
    " \n",
    "    ---------------------------------------------------------------------\n",
    "    4. Since the order of receiving Salaries and Batting samples are indeterministic, we need to use the updateStateByKey function to maintain and update state for each key(yearID).\n",
    "    5. Apply a sort function to every RDD within the DStream, so that we can get the output sorted by yearID.\n",
    "    '''\n",
    " \n",
    "    # filter playerID and distinguish salaries and batting samples\n",
    "    def categorize(fields):\n",
    "      if len(fields) == 5 and fields[3] == 'bondsba01':\n",
    "        return fields[0], ('S', ','.join(fields)) #this will return year, ('S', ','.join(fields))\n",
    "      elif fields[0] == 'bondsba01':\n",
    "        return fields[1], ('B', ','.join(fields)) #this will return year, ('B', ','.join(fields))\n",
    "      else:\n",
    "        return None, None\n",
    " \n",
    "    def updateState(new, old):\n",
    "      tmp = None\n",
    "      if new and old:\n",
    "        new.extend(old)\n",
    "        tmp = new\n",
    "      elif new:\n",
    "        tmp = new\n",
    "      elif old:\n",
    "        tmp = old\n",
    " \n",
    "      if tmp and len(tmp) > 1 and type(tmp) is list:\n",
    "        # @todo: determine the order of salaries and batting in the tmp array, then connect them in the order \"salaries + batting\" preceded by join\n",
    "      else:\n",
    "        return tmp\n",
    " \n",
    "    #@todo complete the filter code to filter by yearid\n",
    "    kstream = kafkaStream.map(lambda line: categorize(line[1].strip().split(','))) \\\n",
    "                         .filter(<complete this>) \\ \n",
    "                         .updateStateByKey(updateState) \\\n",
    "                        # @todo: sort RDD data by key(yearID) in ascending order using RDD transformation\n",
    "    \n",
    "    # kstream.repartition(1).saveAsTextFiles('./4_join/output')\n",
    "    #@todo change with your s3 location\n",
    "    kstream.foreachRDD(lambda rdd: rdd.repartition(1).saveAsTextFile('s3://vandy-bigdata-kzw/hw6/4_join.out'))\n",
    " \n",
    "\n",
    "    # Start the streaming context\n",
    "    ssc.start()\n",
    "    ssc.awaitTerminationOrTimeout(120)\n",
    "    ssc.stop()"
   ]
  }
 ]
}