{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wordcount",
      "provenance": [],
      "authorship_tag": "ABX9TyOJorQyj/pzS6A9/rl1esBE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vu-bigdata-2020/lectures/blob/master/06_mapreduce_and_spark/example/spark-example/wordcount.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TqXvJnJjdLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#A simple workd count example\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmHhdiqQho9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Exq5o6RiNr2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#install spark\n",
        "!wget -q https://downloads.apache.org/spark//spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!ls -l\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQiUr_sviyJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Provides findspark.init() to make pyspark importable as a regular library.\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-2.4.5-bin-hadoop2.7\"\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "staDohTqjcss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The entry point to using Spark SQL is an object called SparkSession. It initiates a Spark Application which all the code for that Session will run on\n",
        "# to larn more see https://blog.knoldus.com/spark-why-should-we-use-sparksession/\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Learning_Spark\") \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6JMROZnkIWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#update the file.txt - from data folder\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CPqGnFikMpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from operator import add\n",
        "lines = spark.read.text(\"file.txt\").rdd.map(lambda r: r[0])\n",
        "counts = lines.flatMap(lambda x: x.split(' ')).map(lambda x: (x.lower().strip(\"'() ,-.\"), 1)).reduceByKey(add)\n",
        "output = counts.sortBy(lambda a: a[1],ascending=False)\n",
        "for (word, count) in output.collect():\n",
        "    print(\"%s: %i\" % (word, count))\n",
        "\n",
        "#spark.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}