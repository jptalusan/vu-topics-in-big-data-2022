{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pagerank",
      "provenance": [],
      "authorship_tag": "ABX9TyMdU9any4n+nn0E8se5rASt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vu-topics-in-big-data-2021/examples/blob/main/example-spark/pagerank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TqXvJnJjdLk"
      },
      "source": [
        "#This is the page rank example taken from spark examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmHhdiqQho9u"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Exq5o6RiNr2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a957ebd6-3ea3-4f08-e302-2e1787c24350"
      },
      "source": [
        "#install spark. we are using the one that uses hadoop as the underlying scheduler.\n",
        "!wget -q https://downloads.apache.org/spark//spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!ls -l\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 223372\n",
            "drwxr-xr-x  1 root root      4096 Mar 18 13:36 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 Feb 22 02:11 spark-3.1.1-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 228721937 Feb 22 02:45 spark-3.1.1-bin-hadoop3.2.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQiUr_sviyJb"
      },
      "source": [
        "#Provides findspark.init() to make pyspark importable as a regular library.\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.1.1-bin-hadoop3.2\"\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "staDohTqjcss"
      },
      "source": [
        "#The entry point to using Spark SQL is an object called SparkSession. It initiates a Spark Application which all the code for that Session will run on\n",
        "# to larn more see https://blog.knoldus.com/spark-why-should-we-use-sparksession/\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Learning_Spark\") \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4xisC_gqTt2"
      },
      "source": [
        "# Page Rank example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TFSe_HFqXyt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7431dfe1-999a-46ac-a49e-ea9b42c7c4ce"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/vu-topics-in-big-data-2021/examples/main/example-spark/data/pagerank_data.txt\n",
        "!cat pagerank_data.txt\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-20 22:47:07--  https://raw.githubusercontent.com/vu-topics-in-big-data-2021/examples/main/example-spark/data/pagerank_data.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24 [text/plain]\n",
            "Saving to: ‘pagerank_data.txt’\n",
            "\n",
            "\rpagerank_data.txt     0%[                    ]       0  --.-KB/s               \rpagerank_data.txt   100%[===================>]      24  --.-KB/s    in 0s      \n",
            "\n",
            "2021-03-20 22:47:07 (899 KB/s) - ‘pagerank_data.txt’ saved [24/24]\n",
            "\n",
            "1 2\n",
            "1 3\n",
            "1 4\n",
            "2 1\n",
            "3 1\n",
            "4 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVl83W_Mr0-p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70e337db-75dc-41dd-e1a2-292e872eb21b"
      },
      "source": [
        "lines = spark.read.text(\"pagerank_data.txt\").rdd.map(lambda r: r[0])\n",
        "lines.take(lines.count())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1 2', '1 3', '1 4', '2 1', '3 1', '4 1']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVQVvCoq2Svv"
      },
      "source": [
        "import re\n",
        "def parseNeighbors(urls):\n",
        "    \"\"\"Parses a urls pair string into urls pair.\"\"\"\n",
        "    parts = re.split(r'\\s+', urls)\n",
        "    print (parts)\n",
        "    return parts[0], parts[1]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTXfLOWVs2TK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c16b28e-7221-473b-d605-2031233aed29"
      },
      "source": [
        "# Loads all URLs from input file and initialize their neighbors. \n",
        "# Mapping is transforming each RDD element using a function and returning a new RDD\n",
        "links = lines.map(lambda x: parseNeighbors(x))\n",
        "#so finally you get a set of rows, where each row is a tuple where the target is the first element in tuple\n",
        "# and the src is the second element in the tuple. I said this wrong in the class.\n",
        "links.take(links.count())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('1', '2'), ('1', '3'), ('1', '4'), ('2', '1'), ('3', '1'), ('4', '1')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cs7qHD5YuUAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee4ed497-911f-49e7-96d4-9bffaa4704d1"
      },
      "source": [
        "neighbors = lines.map(lambda urls: parseNeighbors(urls))\n",
        "neighbors.take(links.count())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('1', '2'), ('1', '3'), ('1', '4'), ('2', '1'), ('3', '1'), ('4', '1')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUvBYoaXuorq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "126830c5-6d1e-44f3-f5c2-f5be4e63c27b"
      },
      "source": [
        "#you can see the links as a table as well by printing it as a dataframe\n",
        "neighbors.toDF().show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+\n",
            "| _1| _2|\n",
            "+---+---+\n",
            "|  1|  2|\n",
            "|  1|  3|\n",
            "|  1|  4|\n",
            "|  2|  1|\n",
            "|  3|  1|\n",
            "|  4|  1|\n",
            "+---+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "za4hOHKxd6Zj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48e9ff70-7b4f-4be9-bc84-81667a2e6580"
      },
      "source": [
        "# we will try to group the elements by key \n",
        "linksdistinct=lines.map(lambda urls: parseNeighbors(urls)).distinct().groupByKey()\n",
        "#when you print it - you will see that the element's second column is a result iterable.\n",
        "\n",
        "linksdistinct.take(linksdistinct.count())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('1', <pyspark.resultiterable.ResultIterable at 0x7f33c17a2810>),\n",
              " ('2', <pyspark.resultiterable.ResultIterable at 0x7f33c17a2590>),\n",
              " ('3', <pyspark.resultiterable.ResultIterable at 0x7f33c17a2410>),\n",
              " ('4', <pyspark.resultiterable.ResultIterable at 0x7f33c17a29d0>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwXf-Aj5eLS_"
      },
      "source": [
        "#to see the iterable you can use the following conversion\n",
        "def see_iterable(iterable):\n",
        "    r = []\n",
        "    for v1_iterable in iterable:\n",
        "        for v2 in v1_iterable:\n",
        "            r.append(v2)\n",
        "\n",
        "    return tuple(r)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaJwFbouVpF4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bd64423-70bf-4370-d53c-365c7c3ceda0"
      },
      "source": [
        "#lets see the first element now\n",
        "print(linksdistinct.take(1)[0][0]) #this is the key\n",
        "print(linksdistinct.take(1)[0][1]) #this is the iterable\n",
        "print(linksdistinct.take(1)[0][0],see_iterable(linksdistinct.take(1)[0][1]))\n",
        "#you can see that it contains elements 2, 3,4\n",
        "print(linksdistinct.take(4)[1][0],see_iterable(linksdistinct.take(2)[1][1]))\n",
        "print(linksdistinct.take(4)[2][0],see_iterable(linksdistinct.take(3)[2][1]))\n",
        "print(linksdistinct.take(4)[3][0],see_iterable(linksdistinct.take(4)[3][1]))\n",
        "print(\"compare this to. And see what group by did. It brough all pages that brough to one and collected them\")\n",
        "neighbors.toDF().show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "<pyspark.resultiterable.ResultIterable object at 0x7f33c17a2210>\n",
            "1 ('2', '3', '4')\n",
            "2 ('1',)\n",
            "3 ('1',)\n",
            "4 ('1',)\n",
            "compare this to. And see what group by did. It brough all pages that brough to one and collected them\n",
            "+---+---+\n",
            "| _1| _2|\n",
            "+---+---+\n",
            "|  1|  2|\n",
            "|  1|  3|\n",
            "|  1|  4|\n",
            "|  2|  1|\n",
            "|  3|  1|\n",
            "|  4|  1|\n",
            "+---+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njHGndftRpo5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d31412fd-bacc-4f61-dce2-783a8278beae"
      },
      "source": [
        "#so again ensure linkdistinct is initations\n",
        "linksdistinct=lines.map(lambda urls: parseNeighbors(urls)).distinct().groupByKey()\n",
        "#now we set up a rank matrix using the the 0th element from list of rows: for example linksdistinct.take(4)[1][0] - take all rows, pick first row and pick 0th element, linksdistinct.take(4)[2][0] will take the 0th element from the list that is the second row\n",
        "#thus the following operation will get us a RDD where the rows are 1,2,3,4 and 1.0\n",
        "ranks = linksdistinct.map(lambda url_neighbors: (url_neighbors[0], 1.0))\n",
        " # Loads all URLs with other URL(s) link to from input file and initialize ranks of them to one.\n",
        "print(ranks.collect())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('1', 1.0), ('2', 1.0), ('3', 1.0), ('4', 1.0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5gNc2RHRMIf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4aeb2d7-df3c-455c-9138-cd5e10a6d704"
      },
      "source": [
        "#lets see the join operation. See how it aggregates by key\n",
        "contribs = linksdistinct.join(ranks)\n",
        "contribs.take(linksdistinct.count())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('1', (<pyspark.resultiterable.ResultIterable at 0x7f33c1764890>, 1.0)),\n",
              " ('4', (<pyspark.resultiterable.ResultIterable at 0x7f33c1764cd0>, 1.0)),\n",
              " ('2', (<pyspark.resultiterable.ResultIterable at 0x7f33c16b4f90>, 1.0)),\n",
              " ('3', (<pyspark.resultiterable.ResultIterable at 0x7f33c16b4d10>, 1.0))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSiiZPLNgZY2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbb957c8-acb4-4cb8-921d-d4ec0f319097"
      },
      "source": [
        "#again this is a pyspark iterable. So lets us the previous function.\n",
        "for i in range(4):\n",
        "  print(contribs.take(4)[i][0],see_iterable(contribs.take(4)[i][1][0])) #note that the value is a tuple of iterable and 1.0. hence there is one more index here in the value we pass to see_iterable."
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 ('2', '3', '4')\n",
            "4 ('1',)\n",
            "2 ('1',)\n",
            "3 ('1',)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAzsP5Ckr0CR"
      },
      "source": [
        "#On each iteration, have page p send a contribution of rank(p)/numNeighbors(p) to its neighbors (the pages it has links to).\n",
        "#note if we process 1 ('2', '3', '4')\n",
        "#then the rank of 1/3 is contributed towards the rank of 2,3 and 4\n",
        "#hence if at start rank of 1 is 1.0\n",
        "#then the output of this function will be\n",
        "# 2,1/3\n",
        "# 3,1/3\n",
        "# 4,1/3\n",
        "#note that this is only the first iteration\n",
        "def computeContribs(urls, rank):\n",
        "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
        "    num_urls = len(urls)    \n",
        "    for url in urls:\n",
        "        yield (url, rank/num_urls)\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdHrHx98vfhS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52ac5caa-d75a-4895-8a2d-d2cd6a7eb1f7"
      },
      "source": [
        "#change iterations number to run the algo for multiple times.\n",
        "iterations=1\n",
        "#reinit rank\n",
        "ranks = linksdistinct.map(lambda url_neighbors: (url_neighbors[0], 1.0))\n",
        "\n",
        "from operator import add\n",
        "# Calculates and updates URL ranks continuously using PageRank algorithm.\n",
        "for iteration in range(iterations):\n",
        "        # Calculates URL contributions to the rank of other URLs.\n",
        "        contribs = linksdistinct.join(ranks).flatMap(lambda url_urls_rank: computeContribs(url_urls_rank[1][0] #this first element is the list for example. ('2', '3', '4') \n",
        "                                                                                           , url_urls_rank[1][1])) #the second element is the rank - at the start it is 1.0\n",
        "        #comment the show method if you run for lot of iterations\n",
        "        contribs.toDF().show(truncate=False)\n",
        "\n",
        "        # Re-calculates URL ranks based on neighbor contributions.\n",
        "        #note that updating the rank by multiplying by 0.85 and adding 0.15 is a heuristic\n",
        "        ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)\n",
        "        #comment the show method if you run for lot of iterations\n",
        "        print(\"rank\")\n",
        "        ranks.toDF().show(truncate=False)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------------------+\n",
            "|_1 |_2                |\n",
            "+---+------------------+\n",
            "|2  |0.3333333333333333|\n",
            "|3  |0.3333333333333333|\n",
            "|4  |0.3333333333333333|\n",
            "|1  |1.0               |\n",
            "|1  |1.0               |\n",
            "|1  |1.0               |\n",
            "+---+------------------+\n",
            "\n",
            "rank\n",
            "+---+-------------------+\n",
            "|_1 |_2                 |\n",
            "+---+-------------------+\n",
            "|4  |0.43333333333333335|\n",
            "|1  |2.6999999999999997 |\n",
            "|2  |0.43333333333333335|\n",
            "|3  |0.43333333333333335|\n",
            "+---+-------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJdXlOIfj_oT"
      },
      "source": [
        "#let us now run it 20 times but without prints\n",
        "iterations=20\n",
        "#reinit rank\n",
        "ranks = linksdistinct.map(lambda url_neighbors: (url_neighbors[0], 1.0))\n",
        "# Calculates and updates URL ranks continuously using PageRank algorithm.\n",
        "for iteration in range(iterations):\n",
        "        # Calculates URL contributions to the rank of other URLs.\n",
        "        contribs = linksdistinct.join(ranks).flatMap(lambda url_urls_rank: computeContribs(url_urls_rank[1][0], url_urls_rank[1][1]))\n",
        "        ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)       \n",
        "        "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGqDLR_Kv-Xl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73046e81-b1c1-4bed-ff22-1b7abdb3f4f7"
      },
      "source": [
        "# Collects all URL ranks and dump them to console.\n",
        "#see the lazy operation. it will take longer to execute this cell than the previous cell.\n",
        "for (link, rank) in ranks.collect():\n",
        "   print(\"%s has rank: %s.\" % (link, rank))\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 has rank: 0.7055659824943556.\n",
            "1 has rank: 1.8833020525169324.\n",
            "3 has rank: 0.7055659824943556.\n",
            "4 has rank: 0.7055659824943556.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}